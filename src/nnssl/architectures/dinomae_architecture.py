import torch
from nnssl.experiment_planning.experiment_planners.plan import ConfigurationPlan
from torch import nn
import math
import warnings


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor
    
def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


class DinoMAEProjectionHead(nn.Module):
    def __init__(self, total_channels: int, hidden_dim: int, output_dim: int, norm_op: nn.Module = nn.InstanceNorm1d, bottleneck_dim = 256):
        super(DinoMAEProjectionHead, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Linear(total_channels, hidden_dim),
            norm_op(hidden_dim, affine=False, track_running_stats=False),
            nn.GELU(),
        )
        self.layer2 = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            norm_op(hidden_dim, affine=False, track_running_stats=False),
            nn.GELU(),
        )
        self.layer3 = nn.Sequential(
            nn.Linear(hidden_dim, bottleneck_dim),
        )
        self.apply(self._init_weights)
        self.last_layer = nn.utils.parametrizations.weight_norm(nn.Linear(bottleneck_dim, output_dim, bias=False))
        self.last_layer.parametrizations.weight.original0.data.fill_(1)
        self.last_layer.parametrizations.weight.original0.requires_grad = False
        # self.last_layer = nn.Sequential(
        #     nn.Linear(bottleneck_dim, output_dim, bias=False),
        #     nn.LayerNorm(output_dim),)
        

    def forward(self, x: list[torch.Tensor]) -> torch.Tensor:
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = nn.functional.normalize(x, dim=-1, p=2)
        x = self.last_layer(x)
        return x
    
    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)


class DinoMAEArchitecture(nn.Module):
    def __init__(self, arch: nn.Module,  features: list[int]):
        super(DinoMAEArchitecture, self).__init__()
        self.encoder = arch.encoder
        self.decoder = arch.decoder
        self.adaptive_pool = nn.AdaptiveAvgPool3d((1, 1, 1))


        total_features = sum(features)
        self.projector = DinoMAEProjectionHead(total_features, 2048, 65536, norm_op=nn.InstanceNorm1d)

    def forward(self, x):
        feat = self.encoder(x)
        # flat_feat = torch.concat([self.adaptive_pool(o) for o in feat], dim=1)
        # flat_feat = torch.reshape(flat_feat, (flat_feat.shape[0], -1))
        # proj_feat = self.projector(flat_feat)
        rec = self.decoder(feat)
        return feat, rec

class DinoMAEEvaArchitecture(nn.Module):
    """
    We don't have multiple CNN stages that we can take the features from and concatenate them, so for the transformer
    we only use the features from the (last) output layer.
    """
    def __init__(self, encoder: nn.Module, embed_dim: int):
        super(DinoMAEEvaArchitecture, self).__init__()
        self.encoder = encoder
        self.adaptive_pool = nn.AdaptiveAvgPool3d((1, 1, 1))

        self.projector = DinoMAEProjectionHead(embed_dim, 2048, 2048, norm_op=nn.InstanceNorm1d)

    def forward(self, x):
        out = self.encoder(x)
        flat_out = self.adaptive_pool(out)
        flat_out = torch.reshape(flat_out, (flat_out.shape[0], -1))
        x = self.projector(flat_out)
        return x
